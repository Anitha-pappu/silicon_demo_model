{
  "code": "@with_default_pyplot_backend  # type: ignore  # mypy issue\ndef get_modeling_overview(\n    model: Model,\n    timestamp_column: str,\n    train_data: pd.DataFrame,\n    test_data: pd.DataFrame,\n    model_tuner: tp.Optional[SupportsModelTuner] = None,\n    model_factory: tp.Optional[SupportsModelFactory] = None,\n    baseline_models: tp.Optional[_TBaselineModels] = None,\n    model_performance_config: TRawConfig = None,\n    pdp_section_config: TRawConfig = None,\n    validation_approach_config: TRawConfig = None,\n) -> TRenderingAgnosticDict:\n    \"\"\"\n    Generates multi-level dict of figures for the Performance Report.\n\n    There are 5 main sections in this report:\n        * Model Introduction (model specification, features, target)\n        * Validation Approach (visual of validation schema, features & target split\n         comparisons, correlation heatmaps)\n        * Model Performance (model metrics and core visuals to access model quality)\n        * Residual Analysis (deep dive in potential root causes of low performance)\n        * Feature Importance (deep dive into main performance drivers)\n\n    This report can be viewed in the notebook or\n    used in report generation to produce standalone report file.\n\n    Args:\n        timestamp_column: column name of timestamp\n        train_data: data containing train input features\n        test_data: data containing test input features\n        model: trained model\n        model_tuner: tuner used to find optimal hyperparameters for model\n        model_factory: model factory used to produce model\n        model_performance_config: dict cast to `ModelPerformanceConfig` with attrs:\n            performance_table_sort_by: metric used for sorting in model performance\n             metrics\n            performance_table_sort_order: sorting order\n        baseline_models: mapping from names to models used for comparison with `model`;\n            typically we want to have:\n            * simple reference models here like AR1, previous month average,\n            * reference models from previous iterations.\n             By default, we provide AR1 model in addition to all baseline models passed.\n        pdp_section_config: dict cast to `PDPSectionConfig` with attrs:\n            * max_features_to_display: maximum number of subplots to display\n            * n_point_in_grid: number of points in grid for pdp\n            * grid_calculation_strategy: strategy for grid calculation.\n             Supported values: \"quantiles\", \"uniform\", \"quantiles+uniform\"\n            * y_axis_range_mode: mode for y-axis range default view.\n             Supported values: \"average\", \"all\".\n            * n_sample_to_calculate_predictions: number of samples to sample\n             from data to calculate predictions\n            * random_state: used for producing sampling\n        validation_approach_config: dict cast to `ValidationApproachConfig` with attrs:\n            * sort_feature_comparison_by_shap: sorts train/test feature\n                comparison in validation section by shap importance if true,\n                leaves `features_in` order otherwise\n    Returns:\n        Dictionary of model performance figures\n    \"\"\"\n\n    train_data = train_data.copy()\n    test_data = test_data.copy()\n\n    prediction_column = \"__prediction\"\n    train_data[prediction_column] = model.predict(train_data)\n    test_data[prediction_column] = model.predict(test_data)\n\n    shap_explanation = model.produce_shap_explanation(train_data)\n\n    # todo: add description for each section\n    #  in performance maybe use plots numbering\n    figs = {\n        identifiers.SectionHeader(\n            header_text=\"Model Introduction\",\n            description=section_descriptions.MODEL_INTRODUCTION_DESCRIPTION,\n        ): _get_introduction(model, model_tuner, model_factory),\n        identifiers.SectionHeader(\n            header_text=\"Validation Approach\",\n            description=None,\n        ): _get_validation_approach(\n            model.target,\n            model.features_in,\n            model.get_shap_feature_importance_from_explanation(shap_explanation),\n            timestamp_column,\n            train_data,\n            test_data,\n            validation_config=parse_config(\n                validation_approach_config, ValidationApproachConfig,\n            ),\n        ),\n        identifiers.SectionHeader(\n            header_text=\"Model Performance\",\n            description=None,\n        ): _get_model_performance(\n            timestamp_column,\n            prediction_column,\n            train_data,\n            test_data,\n            model,\n            baseline_models,\n            config=parse_config(model_performance_config, ModelPerformanceConfig),\n        ),\n        identifiers.SectionHeader(\n            header_text=\"Residual Analysis\",\n            description=section_descriptions.RESIDUAL_ANALYSIS_DESCRIPTION,\n        ): _get_residual_analysis(\n            timestamp_column,\n            prediction_column,\n            train_data,\n            test_data,\n            model,\n        ),\n        identifiers.SectionHeader(\n            header_text=\"Feature Importance\",\n            description=None,\n        ): _get_feature_importance(\n            model,\n            train_data,\n            shap_explanation,\n            pdp_config=parse_config(pdp_section_config, PDPSectionConfig),\n        ),\n    }\n    # assigning to variable name figs\n    # for simplicity and readability\n    return figs  # noqa: WPS331\n",
  "filepath": "optimus_set_point_optimization_kedro_app-0.21.0\\src\\reporting\\config.py",
  "parameters": {
    "silica_model.split.split_parameters.datetime_column": "timestamp"
  },
  "run_command": "kedro run --to-nodes=silica_model.generate_performance_figures",
  "inputs": [
    "trained_model",
    "params:silica_model.split.split_parameters.datetime_column",
    "train_data",
    "test_data"
  ],
  "outputs": [
    "silica_model.model_performance_report"
  ]
}