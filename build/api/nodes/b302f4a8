{
  "code": "def check_uplift_model_error_stat_significance(\n    uplift: BaselineUplifts,\n    baseline_errors_mean: float,\n    baseline_errors_std: float,\n    baseline_errors_n: int,\n    baseline_granularity: tp.Optional[str] = None,\n    alternative_hypothesis: tp.Literal[\"two-sided\", \"less\", \"greater\"] = \"greater\",\n    forced_method: tp.Optional[tp.Literal[\"t-test\", \"bootstrap\"]] = None,\n    bootstrap_samples: int = 1000,\n) -> pd.DataFrame:\n    \"\"\"\n    Checks whether the uplift is caused by model errors using a T-test or bootstrapping.\n    It tests whether the uplift values are statistically significantly different from\n    the model errors or are just a result of the model error distribution. If uplift has\n    more than 30 observations or can be proved to be normal, a T-test is used.\n    Otherwise, bootstrapping is used.\n\n    Baseline model results are assumed to be at the same granularity as uplifts.\n\n    This test will conclude average behavior. For example, if we test the \"greater\"\n    hypothesis and the result is statistically significant, we conclude that on average,\n    produced uplifts are greater than the model error and not a consequence of them.\n\n    Args:\n        uplift: Uplifts to check statistical significance for.\n        baseline_errors_mean: Mean of the baseline errors.\n        baseline_errors_std: Standard deviation of the baseline errors.\n        baseline_errors_n: Number of observations of the baseline errors.\n        baseline_granularity: Granularity of the baseline errors. If None, the\n            original granularity of the uplifts will be used.\n        alternative_hypothesis: Alternative hypothesis for the test.\n        forced_method: If not None, the indicated method will be used to test\n            statistical significance.\n        bootstrap_samples: Number of samples to draw from the uplifts.\n\n    Returns:\n        P-value of the test for all data and by group.\n\n    \"\"\"\n    baseline_granularity = baseline_granularity or uplift.original_granularity\n    baseline_granularity = pd.Timedelta(baseline_granularity)\n    baseline_agg_factor = uplift.agg_granularity / baseline_granularity\n\n    uplift_data = drop_uplifts_na_values(uplift.data)\n    method = select_method(uplift_data, forced_method)\n\n    p_values = pd.DataFrame(\n        {\n            \"group\": uplift.group_names,\n            \"p_value\": np.nan,\n        },\n    )\n\n    for group, uplift_group in uplift.group_iterator(dropna=True):\n        if method == \"t-test\":\n            p_values.loc[p_values[\"group\"] == group, \"p_value\"] = (\n                _run_t_test_uplift_no_model_error(\n                    uplift_group,\n                    baseline_errors_mean * baseline_agg_factor,\n                    baseline_errors_std * np.sqrt(baseline_agg_factor),\n                    baseline_errors_n,\n                    alternative_hypothesis,\n                )\n            )\n        elif method == \"bootstrap\":\n            p_values.loc[p_values[\"group\"] == group, \"p_value\"] = (\n                _run_boostrap_uplift_no_model_error(\n                    uplift_group,\n                    baseline_errors_mean * baseline_agg_factor,\n                    baseline_errors_std * np.sqrt(baseline_agg_factor),\n                    baseline_errors_n,\n                    alternative_hypothesis,\n                    bootstrap_samples,\n                )\n            )\n\n    return p_values\n",
  "filepath": "optimus_set_point_optimization_kedro_app-0.21.0\\src\\recommend\\uplift\\statistical_significance\\uplift_model_error_significance.py",
  "parameters": {
    "impact.baseline_uplift_alt": "less"
  },
  "run_command": "kedro run --to-nodes=check_uplift_model_error_significance",
  "inputs": [
    "uplifts",
    "baseline_errors_mean",
    "baseline_errors_std",
    "baseline_errors_n",
    "params:impact.baseline_uplift_alt"
  ],
  "outputs": [
    "sig_uplift_no_model_error"
  ]
}