# Kedro tutorial - set-point optimization


**The set-point-optimization Kedro version is an extension on the base set-point-optimization use case. It is similar to the base set-point-optimization version except that it is structured as a kedro project.** 

There is also an example pipeline included in `src/set_point_optimization_kedro/pipeline_registry.py` which shows you how to use the optimus API functionality in a kedro pipeline. The pipeline is intended as an example and should not be interpreted as a preferred or exhaustive list of settings and functions to use in your project. What settings and functions you use will be dependent on your use-case. 

In case that you use this pipeline as a starting point:
1. Make sure you understand what each function does to the output before running it in the pipeline
2. Be aware that if you are missing a function in the pipeline, the functionality most likely exists in one of the packages. The included notebooks are tutorials for each package.
3. Keep in mind that solving a set-point-optimization use case involves understanding the problem and the data. Simply running the de pipeline out of the box will never work.
4. Be aware that you do not need to use kedro if it does not fit the needs of your client

### `set_point_optimization` base version
For specific questions regarding the set-point-optimization functions,
we refer to the tutorial notebook of the baes-set-point-optimization use case:

[![Tutorial notebook](https://img.shields.io/badge/jupyter-tutorial_notebook-orange?style=for-the-badge&logo=jupyter)](notebooks/set_point_optimization.ipynb)

If this is your first time working with Kedro, we recommend that you first complete the [Kedro Spaceflights tutorial](https://docs.kedro.org/en/stable/tutorial/spaceflights_tutorial.html). The tutorial takes around 2 hours to complete, and will make the use of the set-point optimization Kedro pipelines much easier.

## Installation
To install `set_point_optimization` use case, please follow steps below.

1. Install [Conda](https://www.anaconda.com/products/distribution).<br>

2. Create and activate a new conda environment.

    ```bash
    conda create -n "optimus" python=3.10
    conda activate optimus
    ```

3. Change directories to the root of your use case.

    ```bash
    cd /path/to/set_point_optimization_kedro/
    ```

4. Install requirements of the use case.

    ```bash
    pip install -r src/requirements.txt
    ```

## How to run

To run the default kedro pipeline:

```bash
kedro run
```

To run the other specific pipelines:

```bash
kedro run --pipeline=live_inference
```

```bash
kedro run --pipeline=feature_report
```


## Input files and configuration

**Required datasets:**
1. [input_data](/src/usecases/set_point_optimization_kedro/src/set_point_optimization_kedro/config/catalog/set_point_optimization.yml):  includes all sensor data with a datetime (timestamp) column 
2. [td](/src/packages/optimus_core/src/optimus_core/tag_dict/tag_dict.py): tag dictionary. The tag dictionary is a file used to describe domain knowledge, primarily provided by your client experts. This domain knowledge is used as parameters throughout OptimusAI pipelines, controlling things like control ranges, and dependencies. Each row in the tag dictionary describes a tag; each column in the tag dictionary describes a property of the tag, which can be used during pipeline runs. For more information on the TagDict check out this [notebook tutorial](/src/usecases/set_point_optimization/src/set_point_optimization/notebooks/set_point_optimization.ipynb).

**Configuration**

1. Data: All datasets are configured using the catalog YAML files in the `conf/base/catalog` folder:
    * [`raw.yml`](/src/usecases/set_point_optimization_kedro/src/set_point_optimization_kedro/config/catalog/raw.yml) that defines raw input datasets.
    * [`artifacts.yml`](/src/usecases/set_point_optimization_kedro/src/set_point_optimization_kedro/config/catalog/artifacts.yml) that defines datasets that save objects generated during pipeline runs.
    * Note that this `conf/base/catalog` folder also contains `examples` folder. It contains `.yml` files that can potentially be used **instead** of two files described above.
2. Parameters: All input parameters to the functions can be found in the YAML files in the `conf/base/parameter` folder. 
3. If you want to use `MLflow` experiment tracking (disabled by default), do:
   1. Install the `kedro-mlflow` extra set of dependencies from `oai.modeling`
      ```bash
      pip install "oai.modeling[kedro-mlflow]"
      ```
      Note that it requires access to McKinsey JFrog artifactory.
   2. Let the `kedro-mlflow` plugin to do its automatic initialization tasks
      ```bash
      kedro mlflow init
      ```
   3. Locate `artifacts.yml` file in `conf/base/catalog` folder and replace it (or its content) by `artifacts_mlflow.yml` file from `conf/base/catalog/examples` folder.
   4. In `mlflow.yml` file generated by `kedro mlflow init` command change value of `long_params_strategy` key to `truncate`.
   5. All set â€“ now when you run the kedro pipeline your artifacts would be logged to `MLflow`.
   6. Get acquainted with experiment tracking tutorial provided my `modeling` package for more details on customizing this setup.


## Pipelines

There are three example pipelines that you can find in `src/set_point_optimization_kedro/pipeline_registry.py`:
1. `__default__`: this pipeline runs all steps required to build a solid predictive model for optimization. It includes preprocessing, feature creation, modeling, reporting, and optimization.
2. `inference_pipeline`: this pipeline uses the predictive model created in the default_pipeline and live data to give live optimized set-points
3. `feature_report`: this pipeline creates a report with plots and statistics on selected features. It is ideal for finding first insights on data quality and uni-variate relations to the target variable.


### Default pipeline

This pipeline includes 7 steps.
As a starting point, it assumes you already structured the data for the project in such a way
that you have a single dataframe with the features and a datetime_column.
We recommend making a separate pipeline for this.

1. **Preprocessing steps**: Takes care of standard pre-processing steps, it might be that you do not need all the steps.
2. **Feature factory steps**: This is where you can create all new physics based features. When creating new features, keep in mind to also add them to the tag_dict.
3. **Modeling steps**: This is where you create a predictive model. In the parameters you can change the type of splitter, the model, and the settings for hyperparameter tuning. For experimenting with multiple models, we advise making a separate python file/notebook since this makes it easy to make a loop and build multiple model reports at once.
4. **Reporting steps**: Creates a model report from the predictive model. There is no easy way to use descriptive names in the report. To get descriptive names we advise to make descriptive features in the feature_factory and only use these variables in your model. The pdp plots can be turned on in the pipeline itself; by default, they are turned off because they take a long time to create. 
5. **Recommend steps**: This step optimizes the data-points in test-data for the given objective function and returns the optimized results. 
6. **Export steps**: This step takes the optimized results and formats them such that they can be used for recommendations.

It could be useful to adjust the `pipeline_registry.py` file in such a way that it can also run the sub-pipelines separately, since you will probably be working on each step at a time. You can take the live_inference pipeline as an example.


### Inference pipeline

This pipeline is an example of what an inference pipeline could look like.
It preprocesses the data, creates the features, gets the live data points,
and optimizes the live data points for the objective function.
This mock example uses the test dataset as live data, in real life;
this node would read the latest date from a database.

### Feature report pipeline

This pipeline creates a report with plots and statistics on selected features. It is ideal for finding first insights on data quality and uni-variate relations to the target variable.

Currently, the column names of the data_frame are used for the naming of the features,
and there is no functionality to use descriptive names in the report.
In case you want to use more descriptive names in the report, we recommend renaming the columns of your input data.
An easy way to do this is by adding a renaming node at the start of this pipeline.

## Building Documentation

To generate a documentation for this use case project, please follow the steps below.

1. Install the packages required for building documentation by `sphinx`.

   ```bash
    pip install -r docs/requirements.txt
    ```

2. Run the following command under the root directory of the use case project

   ```bash
   ./docs/build.sh
   ```

3. Once the documentation building process is finished, you can find the documentation under `docs/build/html/` and you can start browsing the document by opening `docs/build/html/index.html` in your browser.


### Update documentation to reflect your adjustment in the usecase
Very likely, you will use this use case as a starting point and build new features or make adaptation upon it. This means that you may want to update the content and strucutre of the documentation accordingly. You can simply modify the following files and rebuild the documentation.

1. `index.rst`: Use this file to update the index structure of your documentation.

2. `build.sh`: Update this file if you want to add or remove any API documentation for the packages.

3. `conf.py`: Update this file if you want to change configuration on how the documentation should be rendered. More details can be found at [Sphinx's documentaion](https://www.sphinx-doc.org/en/master/usage/configuration.html).

4. Update any files that will be included in the documentation according to `index.rst` 